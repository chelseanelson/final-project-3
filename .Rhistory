load(here("results/fitted_tuned_models/tuned_rf.rda"))
load(here("results/fitted_tuned_models/tuned_svm_poly.rda"))
load(here("results/fitted_tuned_models/tuned_svm_radial.rda"))
# combine best and runtime
initial_results <- bind_rows(nbayes,
enet,
mars,
nnet,
svm_radial,
svm_poly,
knn,
rf
) %>%
left_join(runtime) %>% arrange(desc(mean))
models_to_improve_results <- bind_rows(
nnet,
svm_poly,
rf
) %>%
left_join(runtime) %>% arrange(desc(mean))
nnet <- get_best(tuned_nnet, "nnet")
# select best of each
get_best <- function(tuned_model, name) {
show_best(tuned_model, metric = "roc_auc") %>%
mutate(model = name) %>%
slice_head(n = 1) %>%
select(model, mean, std_err)
}
nnet <- get_best(tuned_nnet, "nnet")
svm_radial <- get_best(tuned_svm_radial, "svm_radial")
svm_poly <- get_best(tuned_svm_poly, "svm_poly")
knn <- get_best(tuned_knn, "knn")
rf <- get_best(tuned_rf, "rf")
# bind tictoc results
runtime <- bind_rows(tictoc_nbayes,
tictoc_enet,
tictoc_mars,
tictoc_nnet,
tictoc_svm_radial,
tictoc_svm_poly,
tictoc_knn,
tictoc_rf
) %>%
select(model, runtime)
# combine best and runtime
initial_results <- bind_rows(nbayes,
enet,
mars,
nnet,
svm_radial,
svm_poly,
knn,
rf
) %>%
left_join(runtime) %>% arrange(desc(mean))
models_to_improve_results <- bind_rows(
nnet,
svm_poly,
rf
) %>%
left_join(runtime) %>% arrange(desc(mean))
models_to_improve_results
# bind tictoc results
runtime <- bind_rows(tictoc_nbayes,
tictoc_enet,
tictoc_mars,
tictoc_nnet,
tictoc_svm_radial,
tictoc_svm_poly,
tictoc_knn,
tictoc_rf
) %>%
select(model, runtime)
runtime
# bind tictoc results
runtime <- bind_rows(tictoc_nbayes,
tictoc_enet,
tictoc_mars,
tictoc_nnet,
tictoc_svm_radial,
tictoc_svm_poly,
tictoc_knn,
tictoc_rf
) %>%
select(model, runtime) %>%
mutate(model = case_when(
model == "NNET" ~ "nnet",
TRUE ~ model
))
runtime <- bind_rows(tictoc_nbayes,
tictoc_enet,
tictoc_mars,
tictoc_nnet,
tictoc_svm_radial,
tictoc_svm_poly,
tictoc_knn,
tictoc_rf
) %>%
select(model, runtime) %>%
mutate(model = case_when(
model == "NNET" ~ "nnet",
TRUE ~ model
))
# select best of each
get_best <- function(tuned_model, name) {
show_best(tuned_model, metric = "roc_auc") %>%
mutate(model = name) %>%
slice_head(n = 1) %>%
select(model, mean, std_err)
}
nbayes <- get_best(fit_nbayes, "nbayes")
enet <- get_best(tuned_enet, "enet")
mars <- get_best(tuned_mars, "MARS")
nnet <- get_best(tuned_nnet, "nnet")
svm_radial <- get_best(tuned_svm_radial, "svm_radial")
svm_poly <- get_best(tuned_svm_poly, "svm_poly")
knn <- get_best(tuned_knn, "knn")
rf <- get_best(tuned_rf, "rf")
# combine best and runtime
initial_results <- bind_rows(nbayes,
enet,
mars,
nnet,
svm_radial,
svm_poly,
knn,
rf
) %>%
left_join(runtime) %>% arrange(desc(mean))
initial_results
# write out results (plots, tables)
save(initial_results, file = here("results/initial_results.rda"))
models_to_improve_results <- bind_rows(
nnet,
svm_poly,
rf
) %>%
left_join(runtime) %>% arrange(desc(mean))
models_to_improve_results
save(models_to_improve_results, file = here("results/models_to_improve_results.rda"))
# ACCS Final Project ----
# Assess final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# load testing and fitted data
load(here("data-splitting/hotel_test.rda"))
load(here("results/final_fit.rda"))
# assessing models performance
hotel_pred_prob <- final_fit %>%
predict(hotel_test, type = "prob")
hotel_pred_class <- final_fit %>%
predict(hotel_test)
hotel_test_res <- hotel_test %>%
bind_cols(hotel_pred_prob) %>%
bind_cols(hotel_pred_class) %>%
select(is_canceled, .pred_class, .pred_0, .pred_1)
metrics <- metric_set(roc_auc, accuracy)
hotel_model_metrics <- metrics(hotel_test_res, truth = is_canceled, estimate = .pred_class, .pred_0)
hotel_model_metrics
hidden_units()
penalty()
num_comp()
scale_factor()
degre()
degree()
cost()
#| label: loading-packages
library(here)
library(tidyverse)
library(tidymodels)
library(knitr)
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt() %>% tab_header(
title = md("**Final Model Performance**")
)
read_rds(here("results/performance_table.rds")) %>% gt()
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt()
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt()
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt() %>% tab_header(
title = md("**Final Model Performance**")
read_rds(here("results/performance_table.rds")) %>% gt() %>% tab_header(
title = md("**Final Model Performance**"))
read_rds(here("results/performance_table.rds")) %>% gt()
library(gt)
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt()
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt() %>% tab_header(
title = md("**Final Model Performance**"))
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt() %>% tab_header(
title = md("**Final Model Performance**"))
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% gt() %>% tab_header(
title = md("**Final Model Performance**")) %>% rename(estimate = estinate)
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% rename(estimate = estinate) %>% arrange(desc(mean)) %>% gt() %>% tab_header(
title = md("**Final Model Performance**"))
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% rename(estimate = estinate) %>% arrange(desc(estimate)) %>% gt() %>% tab_header(
title = md("**Final Model Performance**"))
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>%
rename(estimate = estinate) %>% arrange(desc(estimate)) %>%
gt() %>% tab_header(
title = md("**Final Model Performance**"))
confusion_matrix <-
conf_mat(hotel_test_res, truth = is_canceled, estimate = .pred_class)
confusion_matrix
# ACCS Final Project
# Analysis of improved tuned and trained models (comparison)
# Main Assessment Metric : ROC_AUC
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# load in tuned and fitted models
load(here("results/fitted_tuned_models/tuned_nnet_improve.rda"))
load(here("results/fitted_tuned_models/tuned_svm_poly_improve.rda"))
load(here("results/fitted_tuned_models/tuned_rf_improve.rda"))
nnet <- get_best(tuned_nnet_improve, "nnet")
svm_poly <- get_best(tuned_svm_poly, "svm_poly")
rf <- get_best(improve_rf1, "rf")
nnet
show_best(tuned_nnet_improve, metric = "roc_auc")
autoplot(tuned_nnet_improve, metric = "roc_auc")
## Random Forest
show_best(improve_rf1, metric = "roc_auc")
autoplot(improve_rf1, metric = "roc_auc")
## SVM Polynomial
show_best(tuned_svm_poly, metric = "roc_auc")
autoplot(tuned_svm_poly, metric = "roc_auc")
#| label: improved-results
#| echo: false
load(here("results/improved_results.rda"))%>% gt()
#| label: improved-results
#| echo: false
load(here("results/improved_results.rda"))
#| label: improved-results
#| echo: false
load(here("results/improved_results.rda"))
improved_results %>% gt()
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>%
rename(estimate = estinate) %>% arrange(desc(estimate)) %>%
gt() %>% tab_header(
title = md("**Final Model Performance**"))
#| label: improved-results
#| echo: false
load(here("results/improved_results.rda"))
improved_results %>% gt() %>% tab_header(
title = md("**ROC of Improved Models**"),
subtitle = md("`runtime` in seconds")
)
rf <- get_best(improve_rf1, "rf")
rf
# combine best and runtime
improved_results <- bind_rows(nnet,
svm_poly,
rf
) %>%
left_join(runtime) %>% arrange(desc(mean))
improved_results
### write out results (plots, tables)
save(improved_results, file = here("results/improved_results.rda"))
#| label: improved-results
#| echo: false
load(here("results/improved_results.rda"))
improved_results %>% gt() %>% tab_header(
title = md("**ROC of Improved Models**"),
subtitle = md("`runtime` in seconds")
)
#| label: improved-results
#| echo: false
load(here("results/improved_results.rda"))
improved_results %>% gt() %>% tab_header(
title = md("**ROC of Improved Models**"),
subtitle = md("`runtime` in seconds")
)
load(here("results/fitted_tuned_models/tuned_nnet.rda"))
load(here("results/fitted_tuned_models/tuned_rf.rda"))
load(here("results/fitted_tuned_models/tuned_svm_poly.rda"))
nnet_best <- show_best(tuned_nnet, metric = "roc_auc")
nnet_best
rf_best <- show_best(tuned_rf, metric = "roc_auc")
rf_best
svm_poly_best <- show_best(tuned_svm_poly, metric = "roc_auc")
svm_poly_best
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>%
rename(estimate = estinate) %>% arrange(desc(estimate)) %>%
gt() %>% tab_header(
title = md("**Final Model Performance**"))
# ACCS Final Project ----
# Assess final model
# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
# handle common conflicts
tidymodels_prefer()
# load testing and fitted data
load(here("data-splitting/hotel_test.rda"))
load(here("results/final_fit.rda"))
# assessing models performance
hotel_pred_prob <- final_fit %>%
predict(hotel_test, type = "prob")
hotel_pred_class <- final_fit %>%
predict(hotel_test)
hotel_test_res <- hotel_test %>%
bind_cols(hotel_pred_prob) %>%
bind_cols(hotel_pred_class) %>%
select(is_canceled, .pred_class, .pred_0, .pred_1)
metrics <- metric_set(roc_auc, accuracy)
hotel_model_metrics <- metrics(hotel_test_res, truth = is_canceled, estimate = .pred_class, .pred_0)
hotel_model_metrics
performance_table <-
tibble(
metrics = c("ROC_AUC", "Accuracy"),
estinate = hotel_model_metrics %>% pull(.estimate)
)
confusion_matrix <-
conf_mat(hotel_test_res, truth = is_canceled, estimate = .pred_class)
heatmap <- autoplot(confusion_matrix, type = "heatmap") +
labs(title = "Confusion Matrix Heatmap") + theme_minimal() +
scale_fill_gradient(low = "gray", high = "lightblue") +
theme(legend.position = "none")
write_rds(performance_table, here("results/performance_table.rds"))
write_rds(confusion_matrix, here("results/confusion_matrix.rds"))
ggsave(here("results/heatmap.png"))
library(gt)
load(file = here("results/initial_results.rda"))
initial_results |>
gt() |>
tab_header(
title = md("**ROC of Initial Models**"),
subtitle = md("`runtime` in seconds")
)
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>%
rename(estimate = estinate) %>% arrange(desc(estimate)) %>%
gt() %>% tab_header(
title = md("**Final Model Performance**"))
performance_table <-
tibble(
metrics = c("ROC_AUC", "Accuracy"),
estimate = hotel_model_metrics %>% pull(.estimate)
)
performance_table
performance_table <-
tibble(
metrics = c("Accuracy", "ROC_AUC"),
estimate = hotel_model_metrics %>% pull(.estimate)
)
performance_table
# save out results (plot, table)
write_rds(performance_table, here("results/performance_table.rds"))
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>%
rename(estimate = estinate) %>% arrange(desc(estimate)) %>%
gt() %>% tab_header(
title = md("**Final Model Performance**"))
#| label: final-assessment
#| echo: false
read_rds(here("results/performance_table.rds")) %>% arrange(desc(estimate)) %>%
gt() %>% tab_header(
title = md("**Final Model Performance**"))
hotel_model_metrics <- metrics(hotel_test_res, truth = is_canceled, estimate = .pred_class, .pred_0)
hotel_model_metrics
performance_table <-
tibble(
metrics = c("Accuracy", "ROC_AUC"),
estimate = hotel_model_metrics %>% pull(.estimate)
)
performance_table
confusion_matrix <-
conf_mat(hotel_test_res, truth = is_canceled, estimate = .pred_class)
confusion_matrix
roc_curve(hotel_test_res, is_canceled, .pred_0)
roc_curve <- autoplot(roc_curve(hotel_test_res, is_canceled, .pred_0))
roc_curve
ggsave(here("results/roc_curve.png"))
read_chunk(here('rscripts/ensemble/5_assess_ensemble.R'))
read_chunk(here('rscripts/ensemble/5_assess_ensemble.R'))
#| label: tbl-hotel
#| tbl-cap: "Hotel Bookings"
#| echo: false
hotel_small |>
head(100) |>
DT::datatable()
#| label: load-data
#| echo: false
# load downsampled data
hotel_small <- read_rds("data/hotel_small.rds")
#| label: loading-packages
library(here)
library(tidyverse)
library(tidymodels)
library(knitr)
library(gt)
library(stacks)
# handle common conflicts
tidymodels_prefer()
#load ensemble
load(here("results/fitted_tuned_models/ensemble/hotel_fit.rda"))
load(here("results/fitted_tuned_models/ensemble/tuned_nnet.rda"))
load(here("results/fitted_tuned_models/ensemble/tuned_rf.rda"))
load(here("results/fitted_tuned_models/ensemble/tuned_svm_poly.rda"))
#| label: load-data
#| echo: false
# load downsampled data
hotel_small <- read_rds("data/hotel_small.rds")
#| label: tbl-hotel
#| tbl-cap: "Hotel Bookings"
#| echo: false
hotel_small |>
head(100) |>
DT::datatable()
#| label: tbl-hotel
#| echo: false
hotel_small |>
head(100) |>
datatable(caption = "<b>Hotel Bookings</b>")
#| label: loading-packages
library(here)
library(tidyverse)
library(tidymodels)
library(knitr)
library(gt)
library(stacks)
library(DT)
# handle common conflicts
tidymodels_prefer()
#load ensemble
load(here("results/fitted_tuned_models/ensemble/hotel_fit.rda"))
load(here("results/fitted_tuned_models/ensemble/tuned_nnet.rda"))
load(here("results/fitted_tuned_models/ensemble/tuned_rf.rda"))
load(here("results/fitted_tuned_models/ensemble/tuned_svm_poly.rda"))
#| label: tbl-hotel
#| echo: false
hotel_small |>
head(100) |>
datatable(caption = "<b>Hotel Bookings</b>")
ggsave(here("images/correlation_plot.png"), cor_plot)
# ACCS EDA ----
# In-Depth EDA after split
# BE AWARE: there are random processes in this script (seed set right before them)
# Load package(s)
library(tidymodels)
library(tidyverse)
library(here)
library(naniar)
library(corrplot)
# handle common conflicts
tidymodels_prefer()
#load training data
load(here("data-splitting/hotel_train.rda"))
# set seed
set.seed(3463452)
hotel_eda <- hotel_train %>% slice_sample(prop = .8)
hotel_eda %>% skimr::skim_without_charts()
tmwr_cols <- colorRampPalette(c("#91CBD765", "#CA225E"))
cor_plot <- hotel_eda |>
ungroup() |>
select(where(is.numeric)) |>
cor() |>
corrplot(col = tmwr_cols(200),
tl.col = "black",
method = "ellipse",
tl.cex = 0.5)
ggsave(here("images/correlation_plot.png"), cor_plot)
read_rds(cor_plot, file = here("images/correlation_plot.rds"))
write_rds(cor_plot, file = here("images/correlation_plot.rds"))
#| label: cor-plot
#| echo: false
read_rds(here("images/correlation_plot.rds"))
#| label: tbl-hotel
#| tbl-cap: "Hotel Bookings"
#| tbl-cap-location: top
datatable(head(hotel_small, 100), options = list(server = TRUE))
#| label: tbl-hotel
#| tbl-cap: "Hotel Bookings"
#| tbl-cap-location: top
datatable(head(hotel_small, 100), options = list(server = TRUE))
#| label: tbl-hotel
#| tbl-cap: 'Hotel Bookings'
#| tbl-cap-location: top
datatable(head(hotel_small, 100), options = list(server = TRUE))
